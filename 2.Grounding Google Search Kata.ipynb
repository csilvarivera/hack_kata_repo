{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Grounding in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you use generative text models to generate content grounded in your own documents and data. This capability lets the model access information at runtime that goes beyond its training data. By grounding model responses in Google Search results or data stores within [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction), LLMs that are grounded in data can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Grounding provides the following benefits:\n",
        "\n",
        "- Reduces model hallucinations (instances where the model generates content that isn't factual)\n",
        "- Anchors model responses to specific information, documents, and data sources\n",
        "- Enhances the trustworthiness, accuracy, and applicability of the generated content\n",
        "\n",
        "In the context of grounding in Vertex AI, you can configure two different sources of grounding:\n",
        "\n",
        "1. Google Search results for data that is publicly available and indexed\n",
        "2. [Data stores in Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/create-datastore-ingest), which can include your own data in the form of website data, unstructured data, or structured data\n",
        "\n",
        "### Allowlisting\n",
        "\n",
        "Some of the features in this sample notebook require access to certain features via an allowlist. [Grounding with Vertex AI Search](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) is available in Public Preview, whereas Grounding with Google Search results is generally available.\n",
        "\n",
        "If you use this service in a production application, you will also need to [use a Google Search entry point](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/grounding-search-entry-points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "- Generate LLM text and chat model responses grounded in Google Search results\n",
        "- Compare the results of ungrounded LLM responses with grounded LLM responses\n",
        "- Create and use a data store in Vertex AI Search to ground responses in custom documents and data\n",
        "- Generate LLM text and chat model responses grounded in Vertex AI Search results\n",
        "\n",
        "This tutorial uses the following Google Cloud AI services and resources:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Configuring the LLM and prompt for various examples\n",
        "- Sending example prompts to generative text and chat models in Vertex AI\n",
        "- Sending example prompts with various levels of grounding (no grounding, web grounding, data store grounding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "Restart the kernel after installing packages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from vertexai.generative_models import (\n",
        "    GenerationResponse,\n",
        "    GenerativeModel,\n",
        "    Tool,\n",
        "    grounding,\n",
        ")\n",
        "from vertexai.preview.generative_models import grounding as preview_grounding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cf2dd17690"
      },
      "source": [
        "Initialize the Gemini model from Vertex AI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "652a8969dd5a"
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(\"gemini-1.5-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff453ac772d4"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dce100cab74"
      },
      "outputs": [],
      "source": [
        "def print_grounding_response(response: GenerationResponse):\n",
        "    \"\"\"Prints Gemini response with grounding citations.\"\"\"\n",
        "    grounding_metadata = response.candidates[0].grounding_metadata\n",
        "\n",
        "    # Citation indices are in byte units\n",
        "    ENCODING = \"utf-8\"\n",
        "    text_bytes = response.text.encode(ENCODING)\n",
        "\n",
        "    prev_index = 0\n",
        "    markdown_text = \"\"\n",
        "\n",
        "    for grounding_support in grounding_metadata.grounding_supports:\n",
        "        text_segment = text_bytes[\n",
        "            prev_index : grounding_support.segment.end_index\n",
        "        ].decode(ENCODING)\n",
        "\n",
        "        footnotes_text = \"\"\n",
        "        for grounding_chunk_index in grounding_support.grounding_chunk_indices:\n",
        "            footnotes_text += f\"[{grounding_chunk_index + 1}]\"\n",
        "\n",
        "        markdown_text += f\"{text_segment} {footnotes_text}\\n\"\n",
        "        prev_index = grounding_support.segment.end_index\n",
        "\n",
        "    if prev_index < len(text_bytes):\n",
        "        markdown_text += str(text_bytes[prev_index:], encoding=ENCODING)\n",
        "\n",
        "    markdown_text += \"\\n----\\n## Grounding Sources\\n\"\n",
        "\n",
        "    if grounding_metadata.web_search_queries:\n",
        "        markdown_text += (\n",
        "            f\"\\n**Web Search Queries:** {grounding_metadata.web_search_queries}\\n\"\n",
        "        )\n",
        "        if grounding_metadata.search_entry_point:\n",
        "            markdown_text += f\"\\n**Search Entry Point:**\\n {grounding_metadata.search_entry_point.rendered_content}\\n\"\n",
        "    elif grounding_metadata.retrieval_queries:\n",
        "        markdown_text += (\n",
        "            f\"\\n**Retrieval Queries:** {grounding_metadata.retrieval_queries}\\n\"\n",
        "        )\n",
        "\n",
        "    markdown_text += \"### Grounding Chunks\\n\"\n",
        "\n",
        "    for index, grounding_chunk in enumerate(\n",
        "        grounding_metadata.grounding_chunks, start=1\n",
        "    ):\n",
        "        context = grounding_chunk.web or grounding_chunk.retrieved_context\n",
        "        if not context:\n",
        "            print(f\"Skipping Grounding Chunk {grounding_chunk}\")\n",
        "            continue\n",
        "\n",
        "        markdown_text += f\"{index}. [{context.title}]({context.uri})\\n\"\n",
        "\n",
        "    display(Markdown(markdown_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e336da7161af"
      },
      "source": [
        "## Example: Grounding with Google Search results\n",
        "\n",
        "In this example, you'll compare LLM responses with no grounding with responses that are grounded in the results of a Google Search. You'll ask a question about a recent hardware release from the Google Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a28ca4abb52"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"You are an expert in astronomy. When is the next solar eclipse in the US?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25955ce5d263"
      },
      "source": [
        "### Text generation without grounding\n",
        "\n",
        "Make a prediction request to the LLM with no grounding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2e348ff93e6"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(PROMPT)\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d7cb7cceb99"
      },
      "source": [
        "### Text generation grounded in Google Search results\n",
        "\n",
        "Now you can add the `tools` keyword arg with a `grounding` tool of `grounding.GoogleSearchRetrieval()` to instruct the LLM to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "The search queries and [Search Entry Point](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/grounding-search-entry-points) are available for each `Candidate` in the response. The helper function `print_grounding_response()` prints the response text with citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d9fb83b0ab9"
      },
      "outputs": [],
      "source": [
        "tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())\n",
        "\n",
        "response = model.generate_content(PROMPT, tools=[tool])\n",
        "\n",
        "print_grounding_response(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3920bb2ac0"
      },
      "source": [
        "Note that the response without grounding only has limited information from the LLM about solar eclipses. Whereas the response that was grounded in web search results contains the most up to date information from web search results that are returned as part of the LLM with grounding request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54562717e2a4"
      },
      "source": [
        "## Example: Grounded chat responses\n",
        "\n",
        "You can also use grounding when working with chat models in Vertex AI. In this example, you'll compare LLM responses with no grounding with responses that are grounded in the results of a Google Search and a data store in Vertex AI Search.\n",
        "\n",
        "You'll ask a question about Vertex AI and a follow up question about managed datasets in Vertex AI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "490cf1ed3399"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"What are managed datasets in Vertex AI?\"\n",
        "PROMPT_FOLLOWUP = \"What types of data can I use?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4e8fa97a38e"
      },
      "source": [
        "### Chat session without grounding\n",
        "\n",
        "Start a chat session and send messages to the LLM with no grounding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f64690348d29"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat()\n",
        "\n",
        "response = chat.send_message(PROMPT)\n",
        "display(Markdown(response.text))\n",
        "\n",
        "response = chat.send_message(PROMPT_FOLLOWUP)\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59783e4f1ce"
      },
      "source": [
        "### Chat session grounded in Google Search results\n",
        "\n",
        "Now you can add the `tools` keyword arg with a grounding tool of `grounding.GoogleSearchRetrieval()` to instruct the chat model to first perform a Google Search with the prompt, then construct an answer based on the web search results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58edb2bd860f"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat()\n",
        "tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())\n",
        "\n",
        "response = chat.send_message(PROMPT, tools=[tool])\n",
        "print_grounding_response(response)\n",
        "\n",
        "response = chat.send_message(PROMPT_FOLLOWUP, tools=[tool])\n",
        "print_grounding_response(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "intro-grounding-gemini.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
